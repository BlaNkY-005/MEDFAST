{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ine7DpWkItuX"
      },
      "source": [
        "# Title: Medicine Recommendation System with Machine Learning\n",
        "\n",
        "# Description:\n",
        "\n",
        "This notebook implements a machine learning system that predicts diseases based on symptom input and provides relevant health recommendations including descriptions, precautions, medications, diet, and workout suggestions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZOPwlEOItub"
      },
      "source": [
        "### Installing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xl8eUS-jItuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee872b8-1bcb-49f7-a1f2-e77abefc19b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fuzzywuzzy\n",
        "!pip install scikit-learn\n",
        "!pip install python-Levenshtein  # Speeds up fuzzy matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AbLlvUHItue"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0NRgS8IvItug"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from fuzzywuzzy import process\n",
        "import pickle\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olrB6DyVItuj"
      },
      "source": [
        "### Loading and Exploring the Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FzlinY9GItuj"
      },
      "outputs": [],
      "source": [
        "# Load the training dataset\n",
        "dataset = pd.read_csv('training.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "9A5M_h8UZPae"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wtcfyDeMItul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59332962-c7ea-465d-9489-4fc177b3b607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (3442, 131), (3442,)\n",
            "Test set shape: (1476, 131), (1476,)\n"
          ]
        }
      ],
      "source": [
        "# Split features and target\n",
        "X = dataset.drop('prognosis', axis=1)\n",
        "y = dataset['prognosis']\n",
        "\n",
        "# Encode disease labels\n",
        "le = LabelEncoder()\n",
        "le.fit(y)\n",
        "Y = le.transform(y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=20)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}, {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pekqRGn0Itup"
      },
      "source": [
        "### Training Multiple Models and Enhanced Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TrdiB5QlItuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4437b3b4-e25e-4d49-dedf-3df2f4bafe11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SVC Model Evaluation:\n",
            "  - Accuracy: 1.0000\n",
            "  - Precision: 1.0000\n",
            "  - Recall: 1.0000\n",
            "  - F1 Score: 1.0000\n",
            "  - Processing Time: 1296.2 ms\n",
            "========================================\n",
            "\n",
            "RandomForest Model Evaluation:\n",
            "  - Accuracy: 1.0000\n",
            "  - Precision: 1.0000\n",
            "  - Recall: 1.0000\n",
            "  - F1 Score: 1.0000\n",
            "  - Processing Time: 786.7 ms\n",
            "========================================\n",
            "\n",
            "KNeighbors Model Evaluation:\n",
            "  - Accuracy: 1.0000\n",
            "  - Precision: 1.0000\n",
            "  - Recall: 1.0000\n",
            "  - F1 Score: 1.0000\n",
            "  - Processing Time: 372.0 ms\n",
            "========================================\n",
            "\n",
            "MultinomialNB Model Evaluation:\n",
            "  - Accuracy: 1.0000\n",
            "  - Precision: 1.0000\n",
            "  - Recall: 1.0000\n",
            "  - F1 Score: 1.0000\n",
            "  - Processing Time: 123.6 ms\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "# Dictionary of models to evaluate\n",
        "models = {\n",
        "    'SVC': SVC(kernel='linear', probability=True),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'KNeighbors': KNeighborsClassifier(n_neighbors=5),\n",
        "    'MultinomialNB': MultinomialNB()\n",
        "}\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
        "X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
        "\n",
        "# Scale data for MultinomialNB\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed), columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(scaler.transform(X_test_imputed), columns=X_test.columns)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(name, model, X_tr, X_te):\n",
        "    start_time = time.time()\n",
        "    model.fit(X_tr, y_train)\n",
        "    predictions = model.predict(X_te)\n",
        "    processing_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, predictions, average='weighted')\n",
        "\n",
        "    print(f\"\\n{name} Model Evaluation:\")\n",
        "    print(f\"  - Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  - Precision: {precision:.4f}\")\n",
        "    print(f\"  - Recall: {recall:.4f}\")\n",
        "    print(f\"  - F1 Score: {f1:.4f}\")\n",
        "    print(f\"  - Processing Time: {processing_time:.1f} ms\")\n",
        "    print(f\"{'='*40}\")\n",
        "\n",
        "    return accuracy, model, processing_time\n",
        "\n",
        "# Store results for comparison\n",
        "model_results = {}\n",
        "\n",
        "# Train and evaluate all models\n",
        "for name, model in models.items():\n",
        "    if name == 'MultinomialNB':\n",
        "        accuracy, fitted_model, proc_time = evaluate_model(name, model, X_train_scaled, X_test_scaled)\n",
        "        model_results[name] = {'accuracy': accuracy, 'model': fitted_model, 'scaled': True, 'time': proc_time}\n",
        "    else:\n",
        "        accuracy, fitted_model, proc_time = evaluate_model(name, model, X_train_imputed, X_test_imputed)\n",
        "        model_results[name] = {'accuracy': accuracy, 'model': fitted_model, 'scaled': False, 'time': proc_time}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQQIGB3iItur"
      },
      "source": [
        "### Find and Save the Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "w864WPrZItur"
      },
      "outputs": [],
      "source": [
        "# Select the best model (MultinomialNB from previous analysis)\n",
        "best_model_name = 'MultinomialNB'\n",
        "best_model = model_results[best_model_name]['model']\n",
        "best_accuracy = model_results[best_model_name]['accuracy']\n",
        "requires_scaling = model_results[best_model_name].get('scaled', False)\n",
        "\n",
        "# Create the components dictionary with preprocessing tools\n",
        "components = {\n",
        "    'model': best_model,\n",
        "    'imputer': imputer,\n",
        "    'label_encoder': le,\n",
        "    'requires_scaling': requires_scaling,\n",
        "    'scaler': scaler if requires_scaling else None,\n",
        "    'feature_names': list(X.columns)\n",
        "}\n",
        "\n",
        "# Save the model and preprocessing components to a pickle file\n",
        "with open('best_model.pkl', 'wb') as file:\n",
        "    pickle.dump(components, file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model components\n",
        "def load_model(model_path='best_model.pkl'):\n",
        "    with open(model_path, 'rb') as file:\n",
        "        components = pickle.load(file)\n",
        "    return components\n",
        "\n",
        "# Check symptom relevance for a disease\n",
        "def check_symptom_relevance(disease, user_symptoms, training_data):\n",
        "    # Get all symptoms for the given disease from training data\n",
        "    disease_symptoms = []\n",
        "    disease_row = training_data[training_data['prognosis'] == disease]\n",
        "\n",
        "    if not disease_row.empty:\n",
        "        # Get symptom columns that are 1 for this disease\n",
        "        disease_symptoms = [col for col in disease_row.columns[:-1]\n",
        "                           if disease_row[col].values[0] == 1]\n",
        "\n",
        "    # Calculate how many of the user's symptoms are common for this disease\n",
        "    common_symptoms = [s for s in user_symptoms if s in disease_symptoms]\n",
        "\n",
        "    # Calculate percentage match\n",
        "    match_percentage = (len(common_symptoms) / len(user_symptoms)) * 100 if user_symptoms else 0\n",
        "\n",
        "    return {\n",
        "        \"common_symptoms\": common_symptoms,\n",
        "        \"match_percentage\": match_percentage\n",
        "    }"
      ],
      "metadata": {
        "id": "Fkzhz-KxYfed"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1mEvBPAItus"
      },
      "source": [
        "### Loading All Necessary Datasets for Recommendation System"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean column names to handle case and space inconsistencies\n",
        "def clean_columns(df):\n",
        "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
        "    return df\n",
        "\n",
        "# Load and clean all relevant datasets\n",
        "def load_datasets():\n",
        "    datasets = {\n",
        "        'description': pd.read_csv('description.csv'),\n",
        "        'precaution': pd.read_csv('precaution.csv'),\n",
        "        'diet': pd.read_csv('diet.csv'),\n",
        "        'medication': pd.read_csv('medication.csv'),\n",
        "        'workout': pd.read_csv('workout.csv'),\n",
        "        'severity': pd.read_csv('Symptom-severity.csv'),\n",
        "        'training': pd.read_csv('training.csv')\n",
        "    }\n",
        "\n",
        "    # Clean all dataframes\n",
        "    for key in datasets:\n",
        "        datasets[key] = clean_columns(datasets[key])\n",
        "\n",
        "    return datasets\n",
        "\n",
        "# Load datasets\n",
        "datasets = load_datasets()\n",
        "\n",
        "# Extract all symptoms from severity dataset for fuzzy matching\n",
        "all_symptoms = datasets['severity']['symptom'].str.lower().str.strip().unique().tolist()"
      ],
      "metadata": {
        "id": "L5Wfr0eFItus"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxSfHZmIItus"
      },
      "source": [
        "### Enhanced Symptom Matching with Feedback"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced symptom matching function with score and feedback\n",
        "def match_symptom(input_symptom, min_score=60):\n",
        "    input_symptom = input_symptom.lower().strip()\n",
        "    match, score = process.extractOne(input_symptom, all_symptoms)\n",
        "\n",
        "    if score > min_score:\n",
        "        return match, score\n",
        "    else:\n",
        "        return None, score\n",
        "\n",
        "# Converts a list of symptoms to a binary feature vector (FIXED: now uses binary values instead of severity weights)\n",
        "def symptoms_to_vector(user_symptoms, feature_names, severity_data):\n",
        "    input_vector = [0] * len(feature_names)\n",
        "\n",
        "    for symptom in user_symptoms:\n",
        "        if symptom in feature_names:\n",
        "            index = feature_names.index(symptom)\n",
        "            input_vector[index] = 1  # Mark symptom as present (1) regardless of severity\n",
        "\n",
        "    return np.array([input_vector])"
      ],
      "metadata": {
        "id": "aKu_wAtiItut"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjDFFBnsItut"
      },
      "source": [
        "### Comprehensive Diagnosis Function with Error Handling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagnose function that handles top alternative diagnoses\n",
        "def diagnose(symptoms_list, components, datasets, top_n=3, min_symptoms=2, min_confidence=40):\n",
        "    try:\n",
        "        # Check if symptoms were provided\n",
        "        if not symptoms_list or all(not s.strip() for s in symptoms_list):\n",
        "            return {\"Error\": \"No symptoms provided. Please enter at least one symptom.\"}\n",
        "\n",
        "        # Access model components\n",
        "        model = components['model']\n",
        "        imputer = components['imputer']\n",
        "        le = components['label_encoder']\n",
        "        scaler = components['scaler']\n",
        "        requires_scaling = components['requires_scaling']\n",
        "        feature_names = components['feature_names']\n",
        "\n",
        "        # Match symptoms using fuzzy matching\n",
        "        matched_symptoms = []\n",
        "        unmatched_symptoms = []\n",
        "\n",
        "        for sym in symptoms_list:\n",
        "            match, score = match_symptom(sym)\n",
        "            if match:\n",
        "                matched_symptoms.append(match)\n",
        "            else:\n",
        "                unmatched_symptoms.append((sym, score))\n",
        "\n",
        "        # Check minimum symptoms requirement\n",
        "        if len(matched_symptoms) < min_symptoms:\n",
        "            return {\"Warning\": f\"Please provide at least {min_symptoms} symptoms for more accurate prediction. Currently matched: {len(matched_symptoms)} symptoms\"}\n",
        "\n",
        "        # Convert symptoms to feature vector (now uses binary values)\n",
        "        vector = symptoms_to_vector(matched_symptoms, feature_names, datasets['severity'])\n",
        "\n",
        "        # Apply preprocessing\n",
        "        vector_imputed = imputer.transform(vector)\n",
        "        if requires_scaling:\n",
        "            vector_processed = scaler.transform(vector_imputed)\n",
        "        else:\n",
        "            vector_processed = vector_imputed\n",
        "\n",
        "        # Get prediction and confidence\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "            proba = model.predict_proba(vector_processed)[0]\n",
        "            # Get top N predictions\n",
        "            top_indices = proba.argsort()[-top_n:][::-1]\n",
        "            top_diseases = [(le.inverse_transform([idx])[0], proba[idx] * 100) for idx in top_indices]\n",
        "\n",
        "            # Primary prediction\n",
        "            pred_index = top_indices[0]\n",
        "            disease = le.inverse_transform([pred_index])[0]\n",
        "            confidence = proba[pred_index] * 100\n",
        "\n",
        "            # Check confidence threshold\n",
        "            if confidence < min_confidence:\n",
        "                return {\"Warning\": f\"Low confidence prediction ({confidence:.1f}%). Please provide more symptoms for better accuracy.\"}\n",
        "        else:\n",
        "            pred_index = model.predict(vector_processed)[0]\n",
        "            disease = le.inverse_transform([pred_index])[0]\n",
        "            confidence = None\n",
        "            top_diseases = [(disease, None)]\n",
        "\n",
        "        # Get disease details for primary prediction\n",
        "        desc = datasets['description'][datasets['description']['disease'] == disease]['description'].values\n",
        "        precautions_list = datasets['precaution'][datasets['precaution']['disease'] == disease].values.flatten()[1:]\n",
        "        meds = datasets['medication'][datasets['medication']['disease'] == disease].values.flatten()[1:]\n",
        "        diets_list = datasets['diet'][datasets['diet']['disease'] == disease].values.flatten()[1:]\n",
        "        workouts_list = datasets['workout'][datasets['workout']['disease'] == disease].values.flatten()[1:]\n",
        "\n",
        "        # Calculate symptom relevance score for the disease\n",
        "        symptom_relevance = check_symptom_relevance(disease, matched_symptoms, datasets['training'])\n",
        "\n",
        "        # Handle mixed types safely by converting to string first\n",
        "        result = {\n",
        "            \"Disease\": disease,\n",
        "            \"Confidence\": f\"{confidence:.1f}%\" if confidence is not None else \"Not available\",\n",
        "            \"Description\": str(desc[0]) if len(desc) else \"No description found.\",\n",
        "            \"Precautions\": [str(p) for p in precautions_list if p and str(p).strip() != \"nan\"],\n",
        "            \"Medications\": [str(m) for m in meds if m and str(m).strip() != \"nan\"],\n",
        "            \"Diet\": [str(d) for d in diets_list if d and str(d).strip() != \"nan\"],\n",
        "            \"Workouts\": [str(w) for w in workouts_list if w and str(w).strip() != \"nan\"],\n",
        "            \"MatchedSymptoms\": matched_symptoms,\n",
        "            \"TopAlternatives\": top_diseases[1:],  # Exclude the primary prediction\n",
        "            \"SymptomRelevance\": symptom_relevance\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"Error\": f\"An error occurred during diagnosis: {str(e)}. Please try again.\"}"
      ],
      "metadata": {
        "id": "hyXaXh-3Ituu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User Interaction and Output Formatting"
      ],
      "metadata": {
        "id": "K8A1PaiKYxA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results function for CLI use\n",
        "def display_results(result):\n",
        "    if \"Error\" in result:\n",
        "        print(f\"\\n ERROR: {result['Error']}\")\n",
        "        return\n",
        "    elif \"Warning\" in result:\n",
        "        print(f\"\\n WARNING: {result['Warning']}\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\" PREDICTED DISEASE: {result['Disease']}\")\n",
        "    print(f\" CONFIDENCE: {result['Confidence']}\")\n",
        "\n",
        "    # Display symptom relevance information\n",
        "    if \"SymptomRelevance\" in result:\n",
        "        match_pct = result[\"SymptomRelevance\"][\"match_percentage\"]\n",
        "        print(f\" SYMPTOM MATCH: {match_pct:.1f}% of your symptoms match this disease\")\n",
        "\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"\\n DESCRIPTION:\")\n",
        "    print(result[\"Description\"])\n",
        "\n",
        "    print(\"\\n PRECAUTIONS:\")\n",
        "    for i, p in enumerate(result[\"Precautions\"], 1):\n",
        "        print(f\"{i}. {p}\")\n",
        "\n",
        "    print(\"\\n MEDICATIONS:\")\n",
        "    if result[\"Medications\"]:\n",
        "        for i, m in enumerate(result[\"Medications\"], 1):\n",
        "            print(f\"{i}. {m}\")\n",
        "    else:\n",
        "        print(\"No specific medications listed. Please consult a healthcare professional.\")\n",
        "\n",
        "    print(\"\\n RECOMMENDED WORKOUTS:\")\n",
        "    if result[\"Workouts\"] and result[\"Workouts\"][0] != \"None\":\n",
        "        for i, w in enumerate(result[\"Workouts\"], 1):\n",
        "            print(f\"{i}. {w}\")\n",
        "    else:\n",
        "        print(\"No specific workouts listed. Rest may be recommended.\")\n",
        "\n",
        "    print(\"\\n DIETARY RECOMMENDATIONS:\")\n",
        "    if result[\"Diet\"] and result[\"Diet\"][0] != \"None\":\n",
        "        for i, d in enumerate(result[\"Diet\"], 1):\n",
        "            print(f\"{i}. {d}\")\n",
        "    else:\n",
        "        print(\"No specific diet recommendations listed.\")\n",
        "\n",
        "    print(\"\\n MATCHED SYMPTOMS:\")\n",
        "    for i, s in enumerate(result[\"MatchedSymptoms\"], 1):\n",
        "        print(f\"{i}. {s}\")\n",
        "\n",
        "    # Display alternative diagnoses\n",
        "    if \"TopAlternatives\" in result and result[\"TopAlternatives\"]:\n",
        "        print(\"\\n ALTERNATIVE POSSIBLE DIAGNOSES:\")\n",
        "        for i, (disease, conf) in enumerate(result[\"TopAlternatives\"], 1):\n",
        "            conf_str = f\"{conf:.1f}%\" if conf is not None else \"N/A\"\n",
        "            print(f\"{i}. {disease} (Confidence: {conf_str})\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\" DISCLAIMER: This is not a medical diagnosis. Please consult a healthcare professional.\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "id": "_hYPkY6LrpYX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interactive User Interface"
      ],
      "metadata": {
        "id": "BIUJdUjCY4ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n WELCOME TO THE DISEASE PREDICTION SYSTEM \\n\")\n",
        "    print(\"This system uses machine learning to predict potential diseases based on symptoms.\")\n",
        "    print(\"Please note this is for educational purposes only and is not a substitute for professional medical advice.\\n\")\n",
        "\n",
        "    # Get user input\n",
        "    user_input = input(\"Enter your symptoms separated by commas (e.g., headache, dizziness, nausea): \").split(',')\n",
        "    user_symptoms = [s.strip() for s in user_input]\n",
        "\n",
        "    # Load model and datasets\n",
        "    components = load_model()\n",
        "    datasets = load_datasets()\n",
        "\n",
        "    # Run diagnosis\n",
        "    result = diagnose(user_symptoms, components, datasets)\n",
        "\n",
        "    # Display results\n",
        "    display_results(result)\n",
        "\n",
        "    print(\"\\nThank you for using the Disease Prediction System. Stay healthy!\")"
      ],
      "metadata": {
        "id": "S5s-7SasItuv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a532ddf-bd93-4733-937e-9bb366d57e00"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " WELCOME TO THE DISEASE PREDICTION SYSTEM \n",
            "\n",
            "This system uses machine learning to predict potential diseases based on symptoms.\n",
            "Please note this is for educational purposes only and is not a substitute for professional medical advice.\n",
            "\n",
            "Enter your symptoms separated by commas (e.g., headache, dizziness, nausea): headache, fever, sweating\n",
            "\n",
            "==================================================\n",
            " PREDICTED DISEASE: Malaria\n",
            " CONFIDENCE: 97.0%\n",
            " SYMPTOM MATCH: 66.7% of your symptoms match this disease\n",
            "==================================================\n",
            "\n",
            " DESCRIPTION:\n",
            "Malaria is a mosquito-borne infectious disease affecting humans and other animals.\n",
            "\n",
            " PRECAUTIONS:\n",
            "1. Consult nearest hospital\n",
            "2. avoid oily food\n",
            "3. avoid non veg food\n",
            "4. keep mosquitos out\n",
            "\n",
            " MEDICATIONS:\n",
            "1. ['Antimalarial drugs', 'Antipyretics', 'Antiemetic drugs', 'IV fluids', 'Blood transfusions']\n",
            "\n",
            " RECOMMENDED WORKOUTS:\n",
            "1. Stay hydrated\n",
            "2. Malaria\n",
            "3. Consume nutrient-rich foods\n",
            "4. Malaria\n",
            "5. Include protein-rich foods\n",
            "6. Malaria\n",
            "7. Consume foods rich in antioxidants\n",
            "8. Malaria\n",
            "9. Limit fatty and greasy foods\n",
            "10. Malaria\n",
            "11. Avoid alcohol and caffeine\n",
            "12. Malaria\n",
            "13. Include vitamin C-rich foods\n",
            "14. Malaria\n",
            "15. Consult a healthcare professional\n",
            "16. Malaria\n",
            "17. Rest and conserve energy\n",
            "18. Malaria\n",
            "19. Gradually resume normal diet\n",
            "\n",
            " DIETARY RECOMMENDATIONS:\n",
            "1. ['Malaria Diet', 'Hydration', 'High-Calorie Diet', 'Soft and bland foods', 'Oral rehydration solutions']\n",
            "\n",
            " MATCHED SYMPTOMS:\n",
            "1. headache\n",
            "2. high_fever\n",
            "3. sweating\n",
            "\n",
            " ALTERNATIVE POSSIBLE DIAGNOSES:\n",
            "1. Typhoid (Confidence: 0.5%)\n",
            "2. Chicken pox (Confidence: 0.5%)\n",
            "\n",
            "==================================================\n",
            " DISCLAIMER: This is not a medical diagnosis. Please consult a healthcare professional.\n",
            "==================================================\n",
            "\n",
            "Thank you for using the Disease Prediction System. Stay healthy!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s68xBm5SItux"
      },
      "source": [
        "### Check Scikit-learn Version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(f\"scikit-learn version: {sklearn.__version__}\")"
      ],
      "metadata": {
        "id": "Kk_3eDEGayZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c12055-42a6-499b-abd1-b4439fe32e03"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scikit-learn version: 1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing for Flask Implementation"
      ],
      "metadata": {
        "id": "LeIfrgJ6azYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify, render_template\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the model once when starting the server\n",
        "components = pickle.load(open('best_model.pkl', 'rb'))\n",
        "model = components['model']\n",
        "imputer = components['imputer']\n",
        "le = components['label_encoder']\n",
        "scaler = components['scaler']\n",
        "requires_scaling = components['requires_scaling']\n",
        "feature_names = components['feature_names']\n",
        "datasets = load_datasets()\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    symptoms = request.form.getlist('symptoms')\n",
        "    result = diagnose(symptoms, components, datasets)\n",
        "    return jsonify(result)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "HMNsaLx3az9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf363e4-ba62-4ba6-da4d-c0a001a04b6e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}